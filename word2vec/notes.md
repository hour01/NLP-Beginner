# 语言模型

语言模型是这样一个模型：对于任意的词序列，它能够计算出这个序列是一句话的概率。（说的是不是人话）

### 数学形式：
设s为对应语言的字符串，p(s)为s是一句话的概率，其中s可以表示为
$s=w_1,w_2,w_3,...,w_m$
$w_i$代表一个词
$p(s)=p(w_1,w_2,w_3,...,w_m)=p(w_1)p(w_2|w_1)p(w_3|w_1w_2)...p(w_m|w_1w_2w_3...w_{m-1})$
假设训练集有N个句子，序列(w_1,w_2,w_3,...,w_m)出现次数为n
$p(w_1,w_2,w_3,...,w_m)=\frac{n}{N}$
这个模型预测能力很差，如果单词序列没有出现在训练集中，那么输出概率就是0
>从文本生成角度来看，我们也可以给出如下的语言模型定义：给定一个短语(一个词组或一句话)，语言模型可以生成(预测)接下来的一个词。
## N-gram 语言模型
为了简化$p(w_i|w_1,w_2,w_3,...,w_{i-1})$的计算，引入一阶马尔可夫假设，即每个词只依赖于前一个词，即$p(w_i|w_1,w_2,w_3,...,w_{i-1})≈p(w_i|w_{i-1})$
此时$p(w_1,w_2,w_3,...,w_m)≈p(w_1)\prod_{i=2}^{n}p(w_i|w_{i-1})$

引入二阶马尔可夫假设，即每个词只依赖于前两个词，即$p(w_i|w_1,w_2,w_3,...,w_{i-1})≈p(w_i|w_{i-2}w_{i-1})$
其中，`count(*)`代表`*`在训练集中出现次数
$p(w_i|w_{i-2}w_{i-1})=\frac{count(w_{i-2}w_{i-1}w_i)}{count(w_{i-2}w_{i-1})}$

但基于统计(count)的语言模型还是存在count=0的情况，为了解决这样的问题，可以采用平滑方法
同时，语言模型的关键也转化为如何计算$P(w_i|w_jw_kw_l...)$
# understanding of word2vec


>"You shall know a word by the company it keeps"(J. R. Firth)

使用one-hot对文字进行表示有很多缺点，例如当词典很大时，对应词向量维度也很大，表示出来的句子矩阵非常稀疏；同时词与词之间没有语音的相似性。

而word2vec将每个词用一个低维向量表示，每个词嵌入到语义空间中，语义相似的词对应的词向量也相似，一定程度解决了上述两个问题。

<div align="center">
<img src=./notes_fig/cbow_skipgram.png width=70% />
</div>

主要有两种模型，CBOW和Skip-gram，是类似于ngram的神经语言模型，他们都可以用于计算

$P(w_t|w_iw_jw_kw_l...)$
其中CBOW为：
$P(w_t|...w_{t-2}w_{t-1}w_{t+1}w_{t+2}...)$
Skip-gram为：
$P(...w_{t-2}w_{t-1}w_{t+1}w_{t+2}...|w_t)$
通过训练模型实现上述任务，就可以得到wordvec


## Skip Gram
训练一个神经网络解决以下任务：
输入一个中心词，给出词典中其他词出现在中心词附近的概率，即
$P(...w_{t-2}w_{t-1}w_{t+1}w_{t+2}...|w_t)$
考虑下图的神经网络，包含两次线性变化，最后的输出层配合softmax得到每个词的概率。假设我们需要训练的embedding-size为100维，词典大小10,000，那么中间隐藏层的维度就是100，神经网络第一个隐藏层的权重矩阵就是(10000, 100)，这其中就包含所训练词典中每个词的词向量（考虑列向量形式的one-hot × 权重矩阵）。
<div align="center">
<img src=./notes_fig/model.png width=70% />
</div>

更具体来说，如下图

<div align="center">
<img src=./notes_fig/near_by_words.png width=90% />
</div>

把中心词附近的词定义为中心词周围两个词，即以中心词为中心，大小为2*2+1的窗口
给定窗口大小，得到若干个`(中心词，周围词)`对，神经网络需要根据输入的中心词，极大化周围词的概率，相当于多标签分类，每个中心词的标签就是周围词

一般地，记中心词为$w_c$，周围词为$w_o$，窗口大小m，训练集长度T，模型需要最大化任一中心词生成背景词的概率：
$\prod_{t=1}^T \prod_{-m\leq j \leq m,j\neq 0}P(w^{t+j}_o|w_c^t) $
上式的最大似然估计与最小化下式损失函数等价
$-\frac{1}{T}\sum_{t=1}^T \sum_{-m\leq j \leq m,j\neq 0} logP(w^{t+j}_o|w_c^t) $
设某个词的词向量表示为$v_{w}$，则$P(w_I|w_c)$可以表示为
$P(w_o|w_c)=\frac{exp(v_{w_o}^T v_{w_c})}{\sum_{i\in V}exp(v_{w_o}^iT·v_{w_c})} $

但上述方法在训练时计算量非常大，跟新隐藏层和输出层之间的W矩阵需要对词表中每个词的向量进行更新
有两种优化方法，Hierarchical Softmax, Negative sampling

### Negative Sampling
直接了当的解决了输出词向量过多的问题(词典中每个词)，只选择一些来进行更新。
在每个中心词对应的训练样本标签中，除了正向标签，还可以加入反向标签
即加入不太可能出现在中心词附近的那些词（噪声词），通过对词典进行随机采样，这样得到的词大概率是中心词的非周围词，通过极小化这些词的输出概率加速训练

若使用$\sigma(x)=\frac{1}{1+exp(-x)}$来表示$w_c$ $w_o$ 同时出现在同一窗口的概率：
$P(w_o|w_c)=\sigma(v_{w_o}^T v_{w_c})$
此时关于$w_c$损失函数可以定义为：
$log\sigma(v_{w_o}^Tv_{w_c}) + \sum_{i=1}^klog\sigma(-v_{w_i}^Tv_{w_c}) $
其中k为选取负样本的数量， $w_i$为负样本



## 实验
实现了Skip Gram模型
其中采用两个nn.Embedding分别编码中心词、周围词和负样本词，分别记为`emb_in`和`emb_out`，相当于上述模型中的两次线性变化
$v_{w_c} = emb\_in(w_c), v_{w_o}=emb\_out(w_o), v_{w_i}=emb\_out(w_i) $
最后取emb_in的参数矩阵作为训练得到的词向量

>数据集：一共15304686个英文单词的语料
词典：选取10000个出现次数最多的英文单词，其中一个为`<UNK>`
词嵌入维度：100
epoch: 2
window_size: 5
k : 15

#### 结果

输入一个单词，选择出余弦距离最近的几个词（最相似）：
<img src=./notes_fig/result_1.png width=70% />
`city`与`town,county`等语义相近

<img src=./notes_fig/result_2.png width=70% />

数字之间也具有相似的语义


